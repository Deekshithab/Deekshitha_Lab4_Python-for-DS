{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83f26a29",
      "metadata": {
        "id": "83f26a29"
      },
      "source": [
        "# Unsupervised Lab Session"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea571d1",
      "metadata": {
        "id": "8ea571d1"
      },
      "source": [
        "## Learning outcomes:\n",
        "- Exploratory data analysis and data preparation for model building.\n",
        "- PCA for dimensionality reduction.\n",
        "- K-means and Agglomerative Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7f778a",
      "metadata": {
        "id": "fd7f778a"
      },
      "source": [
        "## Problem Statement\n",
        "Based on the given marketing campigan dataset, segment the similar customers into suitable clusters. Analyze the clusters and provide your insights to help the organization promote their business."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b58f8f",
      "metadata": {
        "id": "33b58f8f"
      },
      "source": [
        "## Context:\n",
        "- Customer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n",
        "- Customer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "867166aa",
      "metadata": {
        "id": "867166aa"
      },
      "source": [
        "## About dataset\n",
        "- Source: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis?datasetId=1546318&sortBy=voteCount\n",
        "\n",
        "### Attribute Information:\n",
        "- ID: Customer's unique identifier\n",
        "- Year_Birth: Customer's birth year\n",
        "- Education: Customer's education level\n",
        "- Marital_Status: Customer's marital status\n",
        "- Income: Customer's yearly household income\n",
        "- Kidhome: Number of children in customer's household\n",
        "- Teenhome: Number of teenagers in customer's household\n",
        "- Dt_Customer: Date of customer's enrollment with the company\n",
        "- Recency: Number of days since customer's last purchase\n",
        "- Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n",
        "- MntWines: Amount spent on wine in last 2 years\n",
        "- MntFruits: Amount spent on fruits in last 2 years\n",
        "- MntMeatProducts: Amount spent on meat in last 2 years\n",
        "- MntFishProducts: Amount spent on fish in last 2 years\n",
        "- MntSweetProducts: Amount spent on sweets in last 2 years\n",
        "- MntGoldProds: Amount spent on gold in last 2 years\n",
        "- NumDealsPurchases: Number of purchases made with a discount\n",
        "- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n",
        "- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n",
        "- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n",
        "- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n",
        "- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n",
        "- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n",
        "- NumWebPurchases: Number of purchases made through the company’s website\n",
        "- NumCatalogPurchases: Number of purchases made using a catalogue\n",
        "- NumStorePurchases: Number of purchases made directly in stores\n",
        "- NumWebVisitsMonth: Number of visits to company’s website in the last month"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a830406",
      "metadata": {
        "id": "5a830406"
      },
      "source": [
        "### 1. Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65c5528",
      "metadata": {
        "id": "d65c5528"
      },
      "outputs": [],
      "source": [
        "### Import required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TpOfi1fyLFID"
      },
      "id": "TpOfi1fyLFID",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c80eb960",
      "metadata": {
        "id": "c80eb960"
      },
      "source": [
        "### 2. Load the CSV file (i.e marketing.csv) and display the first 5 rows of the dataframe. Check the shape and info of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caebc10",
      "metadata": {
        "id": "1caebc10"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/marketing.csv')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head(5))\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(\"Shape:\", df.shape)\n",
        "\n",
        "# Check the info of the dataset\n",
        "print(\"Info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef75724",
      "metadata": {
        "id": "9ef75724"
      },
      "source": [
        "### 3. Check the percentage of missing values? If there is presence of missing values, treat them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c231df",
      "metadata": {
        "id": "f2c231df"
      },
      "outputs": [],
      "source": [
        "# Check the percentage of missing values\n",
        "missing_percentages = df.isnull().mean() * 100\n",
        "\n",
        "# Display the missing percentages\n",
        "print(\"Missing Value Percentages:\")\n",
        "print(missing_percentages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f3709e",
      "metadata": {
        "id": "86f3709e"
      },
      "source": [
        "### 4. Check if there are any duplicate records in the dataset? If any drop them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2970671a",
      "metadata": {
        "id": "2970671a"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate records\n",
        "duplicates = df.duplicated()\n",
        "\n",
        "# Count the number of duplicate records\n",
        "num_duplicates = duplicates.sum()\n",
        "\n",
        "# Display the number of duplicate records\n",
        "print(\"Number of Duplicate Records:\", num_duplicates)\n",
        "\n",
        "# Drop the duplicate records (if any)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Verify if duplicates were dropped\n",
        "print(\"Number of Records after Dropping Duplicates:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a6f2b5a",
      "metadata": {
        "id": "3a6f2b5a"
      },
      "source": [
        "### 5. Drop the columns which you think redundant for the analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ca818b",
      "metadata": {
        "id": "a9ca818b"
      },
      "outputs": [],
      "source": [
        "# Check the column names\n",
        "columns = df.columns\n",
        "\n",
        "# Print the column names\n",
        "print(\"Column Names:\")\n",
        "print(columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of redundant columns to drop\n",
        "redundant_columns = ['ID', 'Dt_Customer']\n",
        "\n",
        "# Drop the redundant columns\n",
        "df.drop(columns=redundant_columns, inplace=True)\n",
        "\n",
        "# Verify the updated DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "N4SHWz24MzDx"
      },
      "id": "N4SHWz24MzDx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4ff0a112",
      "metadata": {
        "id": "4ff0a112"
      },
      "source": [
        "### 6. Check the unique categories in the column 'Marital_Status'\n",
        "- i) Group categories 'Married', 'Together' as 'relationship'\n",
        "- ii) Group categories 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' as 'Single'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb1be519",
      "metadata": {
        "id": "eb1be519"
      },
      "outputs": [],
      "source": [
        "# Check the unique categories in 'Marital_Status'\n",
        "unique_categories = df['Marital_Status'].unique()\n",
        "\n",
        "# Print the unique categories\n",
        "print(\"Unique Categories in 'Marital_Status':\")\n",
        "print(unique_categories)\n",
        "\n",
        "# Group categories 'Married' and 'Together' as 'relationship'\n",
        "df['Marital_Status'].replace(['Married', 'Together'], 'relationship', inplace=True)\n",
        "\n",
        "# Group categories 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' as 'Single'\n",
        "df['Marital_Status'].replace(['Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd'], 'Single', inplace=True)\n",
        "\n",
        "# Verify the updated 'Marital_Status' column\n",
        "print(\"\\nUpdated 'Marital_Status' Column:\")\n",
        "print(df['Marital_Status'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9566bfbe",
      "metadata": {
        "id": "9566bfbe"
      },
      "source": [
        "### 7. Group the columns 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', and 'MntGoldProds' as 'Total_Expenses'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3fa800",
      "metadata": {
        "id": "3c3fa800"
      },
      "outputs": [],
      "source": [
        "# Define the columns to be grouped\n",
        "columns_to_group = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
        "\n",
        "# Create the 'Total_Expenses' column as the sum of the grouped columns\n",
        "df['Total_Expenses'] = df[columns_to_group].sum(axis=1)\n",
        "\n",
        "# Verify the updated DataFrame with the new 'Total_Expenses' column\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0cd083",
      "metadata": {
        "id": "bf0cd083"
      },
      "source": [
        "### 8. Group the columns 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', and 'NumDealsPurchases' as 'Num_Total_Purchases'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c535ede",
      "metadata": {
        "id": "9c535ede"
      },
      "outputs": [],
      "source": [
        "# Define the columns to be grouped\n",
        "columns_to_group = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases']\n",
        "\n",
        "# Create the 'Num_Total_Purchases' column as the sum of the grouped columns\n",
        "df['Num_Total_Purchases'] = df[columns_to_group].sum(axis=1)\n",
        "\n",
        "# Verify the updated DataFrame with the new 'Num_Total_Purchases' column\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d2dca5",
      "metadata": {
        "id": "52d2dca5"
      },
      "source": [
        "### 9. Group the columns 'Kidhome' and 'Teenhome' as 'Kids'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c861a1",
      "metadata": {
        "id": "f7c861a1"
      },
      "outputs": [],
      "source": [
        "# Create the 'Kids' column as the sum of 'Kidhome' and 'Teenhome'\n",
        "df['Kids'] = df['Kidhome'] + df['Teenhome']\n",
        "\n",
        "# Verify the updated DataFrame with the new 'Kids' column\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36f67474",
      "metadata": {
        "id": "36f67474"
      },
      "source": [
        "### 10. Group columns 'AcceptedCmp1 , 2 , 3 , 4, 5' and 'Response' as 'TotalAcceptedCmp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc9109f",
      "metadata": {
        "id": "ecc9109f"
      },
      "outputs": [],
      "source": [
        "# Define the columns to be grouped\n",
        "columns_to_group = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n",
        "\n",
        "# Create the 'TotalAcceptedCmp' column as the sum of the grouped columns\n",
        "df['TotalAcceptedCmp'] = df[columns_to_group].sum(axis=1)\n",
        "\n",
        "# Verify the updated DataFrame with the new 'TotalAcceptedCmp' column\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886bfb08",
      "metadata": {
        "id": "886bfb08"
      },
      "source": [
        "### 11. Drop those columns which we have used above for obtaining new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e853e663",
      "metadata": {
        "id": "e853e663"
      },
      "outputs": [],
      "source": [
        "# Define the columns to drop\n",
        "columns_to_drop = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n",
        "                   'MntGoldProds', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',\n",
        "                   'NumDealsPurchases', 'Kidhome', 'Teenhome', 'AcceptedCmp1', 'AcceptedCmp2',\n",
        "                   'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n",
        "\n",
        "# Drop the columns\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Verify the updated DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ECCofsSMN2mz"
      },
      "id": "ECCofsSMN2mz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4225ced7",
      "metadata": {
        "id": "4225ced7"
      },
      "source": [
        "### 12. Extract 'age' using the column 'Year_Birth' and then drop the column 'Year_birth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d517611e",
      "metadata": {
        "id": "d517611e"
      },
      "outputs": [],
      "source": [
        "# Extract 'age' from 'Year_Birth'\n",
        "df['age'] = pd.Timestamp('now').year - df['Year_Birth']\n",
        "\n",
        "# Drop the 'Year_Birth' column\n",
        "df.drop(columns=['Year_Birth'], inplace=True)\n",
        "\n",
        "# Verify the updated DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d3c92d",
      "metadata": {
        "id": "f2d3c92d"
      },
      "source": [
        "### 13. Encode the categorical variables in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030cfc32",
      "metadata": {
        "id": "030cfc32"
      },
      "outputs": [],
      "source": [
        "# Encode categorical variables using get_dummies()\n",
        "df_encoded = pd.get_dummies(df)\n",
        "\n",
        "# Verify the encoded DataFrame\n",
        "print(df_encoded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9242e36d",
      "metadata": {
        "id": "9242e36d"
      },
      "source": [
        "### 14. Standardize the columns, so that values are in a particular range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72475b68",
      "metadata": {
        "id": "72475b68"
      },
      "outputs": [],
      "source": [
        "# Verify the current columns in the DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the numeric columns\n",
        "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Print the numeric columns\n",
        "print(\"Numeric Columns:\")\n",
        "print(numeric_columns)"
      ],
      "metadata": {
        "id": "6gXYFgZpO6Ne"
      },
      "id": "6gXYFgZpO6Ne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the numeric columns\n",
        "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Calculate the range for each numeric column\n",
        "column_ranges = df[numeric_columns].max() - df[numeric_columns].min()\n",
        "\n",
        "# Specify a threshold for range difference to determine columns to be standardized\n",
        "range_threshold = 100\n",
        "\n",
        "# Identify columns with a range above the threshold\n",
        "columns_to_standardize = column_ranges[column_ranges > range_threshold].index.tolist()\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the selected columns\n",
        "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
        "\n",
        "# Verify the standardized DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "06N7CWatO7DO"
      },
      "id": "06N7CWatO7DO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d063d2e2",
      "metadata": {
        "id": "d063d2e2"
      },
      "source": [
        "### 15. Apply PCA on the above dataset and determine the number of PCA components to be used so that 90-95% of the variance in data is explained by the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df3c70e",
      "metadata": {
        "id": "6df3c70e"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns for PCA analysis\n",
        "columns_for_pca = ['Income', 'Recency', 'NumWebVisitsMonth', 'Complain']\n",
        "\n",
        "# Drop rows with missing values in the selected columns\n",
        "df = df.dropna(subset=columns_for_pca)\n",
        "\n",
        "# Standardize the selected columns\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[columns_for_pca])\n",
        "\n",
        "# Handle missing values (NaN) using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_scaled_imputed = imputer.fit_transform(df_scaled)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(df_scaled_imputed)\n",
        "\n",
        "# Calculate explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Calculate cumulative explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Determine the number of components to explain the desired variance\n",
        "n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
        "\n",
        "# Generate elbow plot\n",
        "plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Elbow Plot for PCA')\n",
        "plt.axvline(x=n_components, color='red', linestyle='--', label='95% Variance')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the number of components required\n",
        "print(\"Number of PCA components to explain 95% variance:\", n_components)\n"
      ],
      "metadata": {
        "id": "F4kcX2ZGSLvE"
      },
      "id": "F4kcX2ZGSLvE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b2df19d7",
      "metadata": {
        "id": "b2df19d7"
      },
      "source": [
        "### 16. Apply K-means clustering and segment the data (Use PCA transformed data for clustering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a8bb4c",
      "metadata": {
        "id": "a3a8bb4c"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns for PCA analysis\n",
        "columns_for_pca = ['Income', 'Recency', 'NumWebVisitsMonth', 'Complain', 'MntWines',\n",
        "                   'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n",
        "                   'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
        "                   'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n",
        "                   'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n",
        "                   'AcceptedCmp2', 'Complain']\n",
        "\n",
        "# Drop rows with missing values in the selected columns\n",
        "df = df.dropna(subset=columns_for_pca)\n",
        "\n",
        "# Standardize the selected columns\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[columns_for_pca])\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=8)  # Use all 8 components for clustering\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Print the PCA components used\n",
        "print(\"PCA Components:\")\n",
        "for i in range(pca.n_components_):\n",
        "    print(f\"Component {i+1}: {pca.explained_variance_ratio_[i]}\")\n",
        "\n",
        "# Apply K-means clustering\n",
        "kmeans = KMeans(n_clusters=4)  # Set the number of clusters\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "# Get the cluster labels\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Add the cluster labels to the original DataFrame\n",
        "df['Cluster'] = cluster_labels\n",
        "\n",
        "# Scatter plot of PCA-transformed data with cluster labels\n",
        "plt.scatter(df_pca[:, 0], df_pca[:, 1], c=cluster_labels)\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('K-means Clustering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n8rxfINlURry"
      },
      "id": "n8rxfINlURry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8463aed",
      "metadata": {
        "id": "d8463aed"
      },
      "source": [
        "### 17. Apply Agglomerative clustering and segment the data (Use Original data for clustering), and perform cluster analysis by doing bivariate analysis between the cluster label and different features and write your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ca165b",
      "metadata": {
        "id": "b5ca165b"
      },
      "outputs": [],
      "source": [
        "# Print the column names\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'ID' and 'Dt_Customer' columns\n",
        "df = df.drop(columns=['ID', 'Dt_Customer'])\n",
        "\n",
        "# Convert 'Year_Birth' to 'Age'\n",
        "df['Age'] = 2023 - df['Year_Birth']\n",
        "\n",
        "# Now drop the 'Year_Birth' column as it's no longer needed\n",
        "df = df.drop(columns=['Year_Birth'])\n",
        "\n",
        "# Define which columns should be encoded vs scaled\n",
        "columns_to_encode = ['Education', 'Marital_Status']\n",
        "columns_to_scale = [col for col in df.columns if col not in columns_to_encode]\n",
        "\n",
        "# Instantiate encoder/scaler\n",
        "scaler = StandardScaler()\n",
        "ohe = OneHotEncoder(drop='first')\n",
        "num_imputer = SimpleImputer(strategy='mean')  # Using mean strategy for imputation\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')  # Using most_frequent strategy for imputation\n",
        "\n",
        "# Combine the encoder/scaler to preprocess data\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('impute_encode', make_pipeline(cat_imputer, ohe), columns_to_encode),\n",
        "        ('impute_scale', make_pipeline(num_imputer, scaler), columns_to_scale)])\n",
        "\n",
        "df_std = preprocess.fit_transform(df)\n",
        "\n",
        "# Applying Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "clusters = agg_clustering.fit_predict(df_std)\n",
        "\n",
        "# Adding cluster labels to the dataframe\n",
        "df['cluster_labels'] = clusters\n",
        "\n",
        "# Listing features and cluster labels\n",
        "print(\"Features: \", df.columns[:-1])  # Excluding 'cluster_labels'\n",
        "print(\"Cluster Labels: \", np.unique(clusters))\n",
        "\n",
        "# For numerical columns, calculate the mean\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "numeric_cluster_analysis = df[numerical_cols].groupby('cluster_labels').mean()\n",
        "\n",
        "# For categorical columns, calculate the mode (most frequent category)\n",
        "categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "categorical_cols_with_labels = list(categorical_cols) + ['cluster_labels']\n",
        "categorical_cluster_analysis = df[categorical_cols_with_labels].groupby('cluster_labels').agg(lambda x: x.mode().iloc[0])\n",
        "\n",
        "# Combine both dataframes for the final cluster analysis\n",
        "cluster_analysis = pd.concat([numeric_cluster_analysis, categorical_cluster_analysis], axis=1)\n",
        "\n",
        "# Printing the cluster analysis\n",
        "cluster_analysis"
      ],
      "metadata": {
        "id": "Dc807TWHVa6A"
      },
      "id": "Dc807TWHVa6A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "797a5ecd",
      "metadata": {
        "id": "797a5ecd"
      },
      "source": [
        "### Visualization and Interpretation of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e75760",
      "metadata": {
        "id": "d1e75760"
      },
      "outputs": [],
      "source": [
        "analysis = \"\"\"\n",
        "Cluster 0:\n",
        "- Average income: $75,886.80\n",
        "- Low number of kids at home and teens at home\n",
        "- Average recency (time since last purchase)\n",
        "- Relatively higher spending on wines, meat products, and fish products\n",
        "- Moderate spending on fruits, sweet products, and gold products\n",
        "- Higher acceptance rates for campaign 3, campaign 4, and campaign 5\n",
        "- Older age group with an average age of 54.90\n",
        "- Mostly have a graduation level of education\n",
        "- Mostly married individuals\n",
        "\n",
        "Cluster 1:\n",
        "- Average income: $34,783.23\n",
        "- High number of kids at home and teens at home\n",
        "- Average recency (time since last purchase)\n",
        "- Lower spending on wines, fruits, meat products, fish products, sweet products, and gold products\n",
        "- Low acceptance rates for campaign 1 and campaign 2\n",
        "- Higher complaint rates\n",
        "- Average age of 51.72\n",
        "- Mostly have a graduation level of education\n",
        "- Mostly married individuals\n",
        "\n",
        "Cluster 2:\n",
        "- Average income: $56,000.80\n",
        "- Moderate number of kids at home and high number of teens at home\n",
        "- Low recency (recent purchases)\n",
        "- Higher spending on wines, meat products, fish products, sweet products, and gold products\n",
        "- Relatively lower acceptance rates for campaign 1, campaign 2, and campaign 5\n",
        "- Lower complaint rates\n",
        "- Average age of 57.02\n",
        "- Mostly have a graduation level of education\n",
        "- Mostly married individuals\n",
        "\n",
        "In conclusion, the three clusters represent distinct customer segments based on their income levels, family size, recency of purchases, spending patterns, responsiveness to marketing campaigns, and demographic characteristics. Cluster 0 represents affluent and older customers who are more responsive to campaigns. Cluster 1 consists of customers with lower income levels, larger family sizes, and lower spending. Cluster 2 represents customers with moderate income levels, moderate family sizes, and higher spending on various product categories. These insights can be valuable for targeted marketing strategies and personalized customer engagement based on specific cluster characteristics.\n",
        "\"\"\"\n",
        "\n",
        "print(analysis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot for continuous features\n",
        "for i in range(df.shape[1] - 1):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x='cluster_labels', y=df.columns[i], data=df)\n",
        "    plt.xlabel('Cluster Labels')\n",
        "    plt.ylabel(df.columns[i])\n",
        "    plt.title(f\"Box Plot: {df.columns[i]} vs Cluster Labels\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OWysufvoW8Ny"
      },
      "id": "OWysufvoW8Ny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "36afd95b",
      "metadata": {
        "id": "36afd95b"
      },
      "source": [
        "-----\n",
        "## Happy Learning\n",
        "-----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "36afd95b"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}